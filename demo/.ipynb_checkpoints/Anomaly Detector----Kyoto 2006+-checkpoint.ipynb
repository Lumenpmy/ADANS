{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "acc88bf6",
   "metadata": {},
   "source": [
    "## Intro\n",
    "This code will show the whole code flow of our proposed ADANS method. For the Kyoto 2006+ dataset, the Anomaly Detector in ADANS uses the AutoEncoder anomaly detection model.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "c89fc9e4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T02:03:41.590900Z",
     "start_time": "2024-10-16T02:03:32.487386Z"
    }
   },
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "#表示每次Import导入的都是最新的模块，在修改代码后不用刷新kernel\n",
    "%matplotlib notebook\n",
    "## import packages\n",
    "import sys\n",
    "sys.path.append('../moudles/')\n",
    "#这个baselines代码中不存在\n",
    "sys.path.append('../')\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import _pickle as pkl\n",
    "from moudles import AE\n",
    "import torch\n",
    "from moudles.ShiftDetector import ShiftDetector\n",
    "from moudles.ShiftAdapter import UNLEARN_adapter\n",
    "from moudles.DANN import DANN\n",
    "import myutils as utils\n",
    "import random\n",
    "from moudles.RepSampleSelector import RepSampleSelector\n",
    "from moudles.ShiftHunter import ShiftHunter\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "e68d4e4b",
   "metadata": {},
   "source": [
    "## Prepare AutoEncoder model and data"
   ]
  },
  {
   "cell_type": "code",
   "id": "50030fb0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T02:04:45.327112Z",
     "start_time": "2024-10-16T02:03:45.060512Z"
    }
   },
   "source": [
    "utils.set_random_seed()\n",
    "feat = np.load('./data/2007.npz')\n",
    "X, y = feat['X'], feat['y']\n",
    "X_ben = X[y==0]\n",
    "train_num=50000\n",
    "X_train = X_ben[:train_num]\n",
    "scaler = MinMaxScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "feature_size=X_train.shape[-1]\n",
    "model,thres= AE.train(X_train,feature_size)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0/10 |Loss: 0.013792422600090504\n",
      "epoch:1/10 |Loss: 0.013447402976453304\n",
      "epoch:2/10 |Loss: 0.013002714142203331\n",
      "epoch:3/10 |Loss: 0.012609494850039482\n",
      "epoch:4/10 |Loss: 0.012191148474812508\n",
      "epoch:5/10 |Loss: 0.011781789362430573\n",
      "epoch:6/10 |Loss: 0.01133252028375864\n",
      "epoch:7/10 |Loss: 0.010742058977484703\n",
      "epoch:8/10 |Loss: 0.009929850697517395\n",
      "epoch:9/10 |Loss: 0.008891576901078224\n",
      "max AD score 0.1436126\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "b6c623d5-968a-436d-a9c2-1b111dba3b73",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T02:06:01.874947Z",
     "start_time": "2024-10-16T02:06:01.584641Z"
    }
   },
   "source": [
    "model_res=model"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "8f799261",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T02:06:14.544461Z",
     "start_time": "2024-10-16T02:06:03.671032Z"
    }
   },
   "source": [
    "FEAT_0 = np.load('./data/2007.npz')\n",
    "X_0, y_0 = scaler.transform(FEAT_0['X']), FEAT_0['y']\n",
    "FEAT_1 = np.load('./data/2011.npz')\n",
    "X_1, y_1 = scaler.transform(FEAT_1['X']), FEAT_1['y']"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "id": "9b84b7e7",
   "metadata": {},
   "source": [
    "## See how AE performs on new data (data where normality shifts occur) and old data (data where normality shifts do not occur)"
   ]
  },
  {
   "cell_type": "code",
   "id": "609001b6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T02:39:01.137542Z",
     "start_time": "2024-10-16T02:38:46.031747Z"
    }
   },
   "source": [
    "\n",
    "print('****************************** Before Normality Shift occurs ******************************')\n",
    "y_pred_0, y_prob_0 = AE.test(model, thres, X_0)\n",
    "#AE.test_plot(y_prob_0,thres, label=y_0)\n",
    "utils.TPR_FPR(y_prob_0, y_0, thres)\n",
    "# utils.multi_metrics(y_prob_0, y_0, thres*1.5)\n",
    "#利用test()函数将每个均方根误差大于thres的样本的y_pred_0设置为1，否则为0，然后y_prob_0表示每个样本的均方根误差"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************** Before Normality Shift occurs ******************************\n",
      "*********************** The relevant test indicators are as follows ***********************\n",
      "FPR: 0.00990333333333333\n",
      "TP 18615\n",
      "FP 2971\n",
      "TN 297029\n",
      "FN 81385\n",
      "Precision: 0.8623644955063468\n",
      "Recall: 0.18615\n",
      "F1_Score: 0.3062030167946968\n",
      "Accuracy: 0.78911\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.00990333333333333"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "64d8383b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T02:39:16.715379Z",
     "start_time": "2024-10-16T02:39:03.701544Z"
    }
   },
   "source": [
    "print('****************************** After Normality Shift occurs ******************************')\n",
    "y_pred_1, y_prob_1 = AE.test(model, thres, X_1)\n",
    "# AE.test_plot(y_prob_1,thres, label=y_1)\n",
    "utils.TPR_FPR(y_prob_1, y_1, thres)\n",
    "# utils.multi_metrics(y_prob_1, y_1, thres*1.5)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************** After Normality Shift occurs ******************************\n",
      "*********************** The relevant test indicators are as follows ***********************\n",
      "FPR: 0.02742675808919362\n",
      "TP 7298\n",
      "FP 8228\n",
      "TN 291771\n",
      "FN 92703\n",
      "Precision: 0.4700502383099317\n",
      "Recall: 0.07297927020729793\n",
      "F1_Score: 0.12634275970119538\n",
      "Accuracy: 0.7476725\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.02742675808919362"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "id": "daea8e65",
   "metadata": {},
   "source": [
    "**Apparently, the new data shows a 14% decrease in the AUC metric and a significant decrease in the performance of the anomaly detection model.\n",
    "Next let's use ADANS to solve the problem of anomaly detection models facing normality shift**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38aa3a1",
   "metadata": {},
   "source": [
    "## Let's use ADANS！"
   ]
  },
  {
   "cell_type": "code",
   "id": "a378f4ec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T02:39:28.571436Z",
     "start_time": "2024-10-16T02:39:19.678312Z"
    }
   },
   "source": [
    "# 新旧数据各自有30万个正常样本，10万个异常样本\n",
    "vali_num = 100000\n",
    "print(len(X_0))\n",
    "X_0_normal=X_0[y_0==0]\n",
    "print(len(X_0_normal))\n",
    "y_0_normal=y_0[y_0==0]\n",
    "y_prob_0_normal=y_prob_0[y_0==0]\n",
    "utils.set_random_seed()\n",
    "# 随机选择10万个样本，旧数据只有正常的，新数据混合有正常和异常的样本\n",
    "random_sequence_o = random.sample(range(0,len(X_0_normal)), vali_num)\n",
    "rmse_o = y_prob_0_normal[random_sequence_o]\n",
    "X_o_normal = X_0_normal[random_sequence_o]\n",
    "y_o_normal=y_0_normal[random_sequence_o]\n",
    "\n",
    "random_sequence_n = random.sample(range(0,len(X_1)), vali_num)\n",
    "X_n = X_1[random_sequence_n]\n",
    "rmse_n = y_prob_1[random_sequence_n]\n",
    "y_n=y_1[random_sequence_n]\n",
    "\n",
    "\n",
    "# Number of anomalous samples included in 100,000 samples of old data\n",
    "j=0\n",
    "for i in range(100000):\n",
    "    if(y_o_normal[i]==1):\n",
    "        j=j+1\n",
    "print(j)\n",
    "# Number of anomalous samples contained in 100,000 samples of incoming data\n",
    "m=0\n",
    "for i in range(100000):\n",
    "    if(y_n[i]==1):\n",
    "        m=m+1\n",
    "print(m)\n",
    "# 选择出的10万个新数据中有24788个异常样本"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400000\n",
      "300000\n",
      "0\n",
      "24788\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "d1575d30-c0de-4624-a6f4-8be4bd4e916e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T02:40:52.300260Z",
     "start_time": "2024-10-16T02:40:51.820188Z"
    }
   },
   "source": [
    "# import time\n",
    "%matplotlib inline\n",
    "\n",
    "# # start_time = time.time()\n",
    "old_num = 50000\n",
    "label_num = 1000\n",
    "labeling_probability=label_num/vali_num\n",
    "# print(labeling_probability)\n",
    "scranner = RepSampleSelector(model, X_o_normal, X_n, y_n, old_num, label_num,X_1)\n",
    "# result = scranner.RMSE_Kmeans()\n",
    "result = scranner.Random_Adapter()\n",
    "# end_time = time.time()\n",
    "# print(f\"执行时间：{end_time - start_time} 秒\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01\n",
      "NOTICE: simulating labelling...\n",
      "Filter 241 anomalies in X_i_rep\n",
      " (label_num:1000, X_i_rep_normal:759, X_i:100000)\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64be299f-6f29-4381-a5bb-ae43c1a3b41b",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ShiftHunter.__init__() missing 3 required positional arguments: 'control_res', 'treatment_res', and 'calibrator'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[9], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m shifthunter\u001B[38;5;241m=\u001B[39mShiftHunter()\n\u001B[0;32m      2\u001B[0m result \u001B[38;5;241m=\u001B[39m shifthunter\u001B[38;5;241m.\u001B[39mexplainer( X_n, y_n,label_num,X_1)\n\u001B[0;32m      3\u001B[0m X_o_rep_nor\u001B[38;5;241m=\u001B[39mresult[\u001B[38;5;241m0\u001B[39m]\n",
      "\u001B[1;31mTypeError\u001B[0m: ShiftHunter.__init__() missing 3 required positional arguments: 'control_res', 'treatment_res', and 'calibrator'"
     ]
    }
   ],
   "source": "print(result)"
  },
  {
   "cell_type": "code",
   "id": "559d8a47-864c-4583-a935-907095af7eff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T02:40:34.456638Z",
     "start_time": "2024-10-16T02:40:34.171381Z"
    }
   },
   "source": [
    "X_o_rep_nor=result[0]\n",
    "X_n_rep_nor=result[1]"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "id": "cd7677ba-26e1-4049-9b7c-a3a8bd97dfcd",
   "metadata": {},
   "source": [
    "## Detection of normality shift using Normality Shift Detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90d5de9-2db6-4162-8880-8952f8dd3ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "X_o_rep_nor_np=X_o_rep_nor.numpy()\n",
    "X_n_rep_nor_np=X_n_rep_nor.numpy()\n",
    "\n",
    "utils.set_random_seed()\n",
    "sd = ShiftDetector(method='min_max')\n",
    "print(\"Amplify differences between values that differ between rmse_o\")\n",
    "_, rmse_o_rep_nor = AE.test(model, thres, X_o_rep_nor_np[:len(X_n_rep_nor_np)])\n",
    "score_o_rep_nor=sd.process(rmse_o_rep_nor)\n",
    "print(\"Amplify differences between values that differ between rmse_n\")\n",
    "_, rmse_n_rep_nor = AE.test(model, thres, X_n_rep_nor_np)\n",
    "score_n_rep_nor=sd.process(rmse_n_rep_nor)\n",
    "t = utils.get_params('ShiftDetector')['test_thres']\n",
    "p_value = sd.Monte_Carlo(score_o_rep_nor,score_n_rep_nor)[0]\n",
    "if p_value >= t:\n",
    "    print(\"No normality shift!\", p_value)\n",
    "else:\n",
    "    print('Shift! P-value is', p_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05dc418d-a829-467d-8ee2-c5bdf8652b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 画出的频率分布直方图和经过光滑设置的折线图\n",
    "# print(\"Visualize Shift:\")\n",
    "# sd.visualize_hists(score_o_rep_nor,score_n_rep_nor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c13739e-2281-4c70-85fe-ee97c9b05439",
   "metadata": {},
   "source": [
    "## Adaption of normality shift using Normality Shift Adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba38580-6d9b-4dd3-be3d-8f65e9f9ea71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_retrain = np.concatenate((X_o_rep_nor,X_n_rep_nor))\n",
    "# retrain_model,retrain_thres=AE.train(X_retrain, X_retrain.shape[-1])\n",
    "# print('****************************** After Retraining ******************************')\n",
    "# y_pred, y_prob = AE.test(retrain_model, retrain_thres, X_1)\n",
    "# utils.TPR_FPR(y_prob, y_1, thres)\n",
    "# utils.multi_metrics(y_prob, y_1, retrain_thres*1.5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1865c0e6-16c8-4df8-844d-059852f2dd05",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# utils.set_random_seed()\n",
    "# dann=DANN(model,X_o_rep_nor,X_n_rep_nor,feature_size,thres*0.01,labeling_probability)\n",
    "# dann.update_AE()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "utils.set_random_seed()\n",
    "UNLEARN_model=UNLEARN_adapter(model,X_o_rep_nor,X_n_rep_nor)\n",
    "%matplotlib notebook\n",
    "print('After OWAD Adaptation @2011:')\n",
    "y_pred, y_prob = AE.test(UNLEARN_model,thres, X_1)\n",
    "AE.test_plot(y_prob,thres, label=y_1)\n",
    "utils.TPR_FPR(y_prob, y_1, thres)\n",
    "utils.multi_metrics(y_prob, y_1, thres*1.5)\n"
   ],
   "id": "562e2516b071b4ae"
  },
  {
   "cell_type": "markdown",
   "id": "e2d37877",
   "metadata": {},
   "source": [
    "## Re-testing the performance of the anomaly detection model (AE) on new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b56ee3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %matplotlib notebook\n",
    "# print('After OWAD Adaptation @2011:')\n",
    "# y_pred, y_prob = AE.test(dann.updated_AE,thres, X_1)\n",
    "# AE.test_plot(y_prob,thres, label=y_1)\n",
    "# utils.TPR_FPR(y_prob, y_1, thres)\n",
    "# utils.multi_metrics(y_prob, y_1, thres*1.5)\n",
    "# # 10万个异常样本，30万个正常样本"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec2e979",
   "metadata": {},
   "source": [
    "**(As we can see that, ADANS improves the performance of AD models from 0.81 to 0.89 with 10k labels, which is 10% of validation set)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5d3ce2",
   "metadata": {},
   "source": [
    "## Contrasting methods: retraining\n",
    "A common practice to tackle concept drift is to retrain the model with both old and new samples. Here we'll show whether retraining works in this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57c448d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils.set_random_seed()\n",
    "# random_sequence_1 = random.sample(range(0,len(X_0)), train_num)\n",
    "# random_sequence_2 = random.sample(range(0,len(X_0)), label_num)\n",
    "# X_retrain = np.concatenate((X_0[random_sequence_1],X_1[random_sequence_2]))\n",
    "# retrain_model,retrain_thres=AE.train(X_retrain, X_retrain.shape[-1])\n",
    "# print('****************************** After Retraining ******************************')\n",
    "# y_pred, y_prob = AE.test(retrain_model, retrain_thres, X_1)\n",
    "# utils.TPR_FPR(y_prob, y_1, thres)\n",
    "# utils.multi_metrics(y_prob, y_1, retrain_thres*1.5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb9be9a-9cdd-46d5-b056-9777d5fa8ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# #Retrain-I\n",
    "# # 只使用新数据中代表性的正常样本训练\n",
    "\n",
    "# utils.set_random_seed()\n",
    "# retrain_model,retrain_thres=AE.train(X_n_rep_nor.numpy(), (X_n_rep_nor.shape[-1]))\n",
    "# print('****************************** After Retraining ******************************')\n",
    "# y_pred, y_prob = AE.test(retrain_model, retrain_thres, X_1)\n",
    "# utils.TPR_FPR(y_prob, y_1, thres)\n",
    "# utils.multi_metrics(y_prob, y_1, retrain_thres*1.5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437ca6d6-2289-4918-a6f2-205cbfc6f1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# #Retrain-II\n",
    "#除去Adpater+Rectrain\n",
    "\n",
    "# utils.set_random_seed()\n",
    "# X_retrain = np.concatenate((X_o_rep_nor,X_n_rep_nor))\n",
    "# retrain_model,retrain_thres=AE.train(X_retrain, X_retrain.shape[-1])\n",
    "# print('****************************** After Retraining ******************************')\n",
    "# y_pred, y_prob = AE.test(retrain_model, retrain_thres, X_1)\n",
    "# utils.TPR_FPR(y_prob, y_1, thres)\n",
    "# utils.multi_metrics(y_prob, y_1, retrain_thres*1.5) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d581fe",
   "metadata": {},
   "source": [
    "**(As we can see that, Retraining actually has a negtive effect. Please refer to our paper for more analysis)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa4f45f-06f3-4940-8d54-6c2ada17f252",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
